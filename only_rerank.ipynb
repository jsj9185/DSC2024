{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DSC/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch, gc\n",
    "from importlib import reload\n",
    "\n",
    "import financerag.tasks.Base_Task; reload(financerag.tasks.Base_Task)\n",
    "import financerag.tasks.ConvFinQATask; reload(financerag.tasks.ConvFinQATask)\n",
    "import financerag.tasks.FinanceBenchTask; reload(financerag.tasks.FinanceBenchTask)\n",
    "import financerag.tasks.FinDERTask; reload(financerag.tasks.FinDERTask)\n",
    "import financerag.tasks.FinQATask; reload(financerag.tasks.FinQATask)\n",
    "import financerag.tasks.FinQABenchTask; reload(financerag.tasks.FinQABenchTask)\n",
    "import financerag.tasks.MultiHierttTask; reload(financerag.tasks.MultiHierttTask)\n",
    "import financerag.tasks.TATQATask; reload(financerag.tasks.TATQATask)\n",
    "reload(financerag.tasks)\n",
    "\n",
    "from financerag.rerank import CrossEncoderReranker\n",
    "from financerag.retrieval import DenseRetrieval, SentenceTransformerEncoder, BM25Retriever, HybridRetriever\n",
    "from financerag.tasks import ConvFinQA, FinanceBench, FinDER, FinQA, FinQABench, MultiHiertt, TATQA\n",
    "\n",
    "# Setup basic logging configuration to show info level messages.\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize FinDER Task\n",
    "\n",
    "convfinqa_task = ConvFinQA()\n",
    "finbench_task = FinanceBench()\n",
    "finder_task = FinDER()\n",
    "finqa_task = FinQA()\n",
    "finqabench_task = FinQABench()\n",
    "multih_task = MultiHiertt()\n",
    "tatqa_task = TATQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄임말 확장\n",
    "# 테이블을 instruction: 테이블 컴포넌트를 뽑아달라고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 8\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize DenseRetriever model\n",
    "\n",
    "# You can replace 'intfloat/e5-large-v2' with any other model supported by SentenceTransformers.\n",
    "# For example: 'BAAI/bge-large-en-v1.5', 'Linq-AI-Research/Linq-Embed-Mistral', etc.\n",
    "base_encoder = \"BAAI/bge-m3\" # \"intfloat/multilingual-e5-large-instruct\"  #\"BAAI/bge-large-en-v1.5\" #\"nvidia/NV-Embed-v2\"(20GB) \"intfloat/e5-mistral-7b-instruct\"(9GB) \n",
    "                                            #\"dunzhang/stella_en_1.5B_v5\" (6GB)  \"jinaai/jina-embeddings-v3\"(1.1GB) \"jinaai/jina-embeddings-v2-base-code\"(320MB)\n",
    "encoder_model = SentenceTransformerEncoder(\n",
    "    model_name_or_path=base_encoder,\n",
    "    query_prompt='query: ',\n",
    "    doc_prompt='passage: '\n",
    ")\n",
    "\n",
    "retrieval_model = DenseRetrieval(\n",
    "     model=encoder_model\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# bm25_retriever = BM25Retriever()\n",
    "# dense_retriever = DenseRetrieval(model=encoder_model)\n",
    "\n",
    "# # Initialize the hybrid retriever\n",
    "# hybrid_retriever = HybridRetriever(\n",
    "#     lexical_retriever=bm25_retriever,\n",
    "#     dense_retriever=dense_retriever,\n",
    "#     lexical_weight=0.4,\n",
    "#     dense_weight=0.6\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ConvfinQA Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f40bf569c94c80a68da4c7f70dc3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb5c86b4a3644558f7b3e0634d62ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on FinBench Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58edd5f56bfb42deab266e47d48031ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbf6076b5d94bc6a0a0c7aae8d0e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on FinDER Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87edc645d71149bebfe01bd2ae83fc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b4c66ecd974a3e880cc0586a679203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on FinQA Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a495178edfc64136b67ea2be13d7defe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85439caad634a13a89b582c65665d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on FinQABench Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df699be48cdd4e35bcd8e57058f7aab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cdf532fd8a413d96bf78ede9c2a030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on MultiHiertt Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fa4fb6e5a14e2bb684222529a5a531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7061122180a4449b65a5f9f78a7ea23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on TATQA Task\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e04205285c0491799b821526488c770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.retrieval.dense:Sorting corpus by document length...\n",
      "INFO:financerag.retrieval.dense:Encoding corpus in batches... This may take a while.\n",
      "INFO:financerag.retrieval.dense:Encoding batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e4adecf2ab44478ee38fe1a8c11cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Perform retrieval\n",
    "\n",
    "print(\"Working on ConvfinQA Task\")\n",
    "convfinqa_result = convfinqa_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinBench Task\")\n",
    "finbench_result = finbench_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinDER Task\")\n",
    "finder_result = finder_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinQA Task\")\n",
    "finqa_result = finqa_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on FinQABench Task\")\n",
    "finqabench_result = finqabench_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on MultiHiertt Task\")\n",
    "multih_result = multih_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Working on TATQA Task\")\n",
    "tatqa_result = tatqa_task.retrieve(\n",
    "    retriever=retrieval_model)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "results = [\n",
    "    convfinqa_result,\n",
    "    finbench_result,\n",
    "    finder_result,\n",
    "    finqa_result,\n",
    "    finqabench_result,\n",
    "    multih_result,\n",
    "    tatqa_result\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print a portion of the retrieval results to verify the output.\n",
    "# for result in results:\n",
    "#     print(f\"Retrieved results for {len(result)} queries. Here's an example of the top 5 documents for the first query:\")\n",
    "\n",
    "# for result in results:\n",
    "#     for q_id, res in result.items():\n",
    "#         print(f\"\\nQuery ID: {q_id}\")\n",
    "#         # Sort the result to print the top 5 document ID and its score\n",
    "#         sorted_results = sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#         for i, (doc_id, score) in enumerate(sorted_results[:5]):\n",
    "#             print(f\"  Document {i + 1}: Document ID = {doc_id}, Score = {score}\")\n",
    "\n",
    "#         break  # Only show the first query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evalset(dataset_name):\n",
    "    qrels = {}\n",
    "    df_qrels = pd.read_csv(f\"./data/test/{dataset_name}_qrels.tsv\", sep='\\t')\n",
    "    for _, row in df_qrels.iterrows():\n",
    "        if row['query_id'] not in qrels:\n",
    "            qrels[row['query_id']] = {}\n",
    "        qrels[row['query_id']][row['corpus_id']] = row['score']\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    convfinqa_task,\n",
    "    finbench_task,\n",
    "    finder_task,\n",
    "    finqa_task,\n",
    "    finqabench_task,\n",
    "    multih_task,\n",
    "    tatqa_task\n",
    "]\n",
    "\n",
    "qrels = [\n",
    "    get_evalset('ConvFinQA'),\n",
    "    get_evalset('FinanceBench'),\n",
    "    get_evalset('FinDER'),\n",
    "    get_evalset('FinQA'),\n",
    "    get_evalset('FinQABench'),\n",
    "    get_evalset('MultiHeirtt'),\n",
    "    get_evalset('TATQA')\n",
    "]\n",
    "\n",
    "# for qrel, task in zip(qrels, tasks):\n",
    "#     metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "#     retrieve_ndcgs.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:NDCG@10: 0.3429\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvFinQA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinanceBench\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTATQA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qrel, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qrels, tasks):\n\u001b[0;32m---> 20\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(qrels\u001b[38;5;241m=\u001b[39mqrel, results\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mretrieve_results, k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     22\u001b[0m     ndcg_values\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG@10\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# NDCG@10\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     map_values\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAP@10\u001b[39m\u001b[38;5;124m'\u001b[39m])    \u001b[38;5;66;03m# MAP@10\u001b[39;00m\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:448\u001b[0m, in \u001b[0;36mBaseTask.evaluate\u001b[0;34m(qrels, results, k_values, ignore_identical_ids)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Compute the average scores for each k\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[0;32m--> 448\u001b[0m     ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    449\u001b[0m     _map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(_map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    450\u001b[0m     recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ndcg_values = []\n",
    "map_values = []\n",
    "recall_values = []\n",
    "precision_values = []\n",
    "\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "\n",
    "    metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "\n",
    "    ndcg_values.append(metrics[0]['NDCG@10'])  # NDCG@10\n",
    "    map_values.append(metrics[1]['MAP@10'])    # MAP@10\n",
    "    recall_values.append(metrics[2]['Recall@10'])  # Recall@10\n",
    "    precision_values.append(metrics[3]['P@10'])  # P@10\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(ndcg_values, marker='o', label='NDCG@10', color='b')\n",
    "plt.title('NDCG@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(map_values, marker='o', label='MAP@10', color='g')\n",
    "plt.title('MAP@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(recall_values, marker='o', label='Recall@10', color='r')\n",
    "plt.title('Recall@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(precision_values, marker='o', label='P@10', color='c')\n",
    "plt.title('Precision@10')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"jinaai/jina-reranker-v2-base-multilingual\",\n",
    "            \"Alibaba-NLP/gte-multilingual-reranker-base\",\n",
    "        \"BAAI/bge-reranker-v2-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize CrossEncoder Reranker\n",
    "# base_reranker1 = \"jinaai/jina-reranker-v2-base-multilingual\"  # Model name\n",
    "# reranker1 = CrossEncoderReranker(\n",
    "#     model=CrossEncoder(base_reranker1, trust_remote_code=True)  # Ensure trust_remote_code is enabled\n",
    "# )\n",
    "\n",
    "base_reranker = \"BAAI/bge-reranker-v2-m3\" #\"BAAI/bge-reranker-base\" #m2 \n",
    "\n",
    "reranker = CrossEncoderReranker(\n",
    "    model=CrossEncoder(base_reranker)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Reranking (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on ConvFinQA Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3644d3f0404b1fbcc3bb511e9a64a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinBench Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b35e31c1854bec8a71da388585d038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinDER Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa0d94a475548b287a8fe3f52f9d035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinQA Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7559b01983f7424881ddd43f1e29f4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinQABench Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937613e0735a4d98b3afa14b326f9fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on MultiHiertt Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93f3f34883b44b99b8eb989abc72453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on TATQA Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-100....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704253d61443435abc6a1bc97d00e19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20788 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Perform rerankin\n",
    "\n",
    "top_k= 100 # Number of Reranking results\n",
    "batch_size = 8 # 32\n",
    "\n",
    "print(\"\\nWorking on ConvFinQA Reranking\")\n",
    "convfinqa_rerank = convfinqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=convfinqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinBench Reranking\")\n",
    "finbench_rerank = finbench_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finbench_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinDER Reranking\")\n",
    "finder_rerank = finder_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finder_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQA Reranking\")\n",
    "finqa_rerank = finqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQABench Reranking\")\n",
    "finqabench_rerank = finqabench_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=finqabench_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on MultiHiertt Reranking\")\n",
    "multih_rerank = multih_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=multih_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on TATQA Reranking\")\n",
    "tatqa_rerank = tatqa_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=tatqa_result,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "reranking_results = [\n",
    "    convfinqa_rerank,\n",
    "    finbench_rerank,\n",
    "    finder_rerank,\n",
    "    finqa_rerank,\n",
    "    finqabench_rerank,\n",
    "    multih_rerank,\n",
    "    tatqa_rerank\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, rerank_result in enumerate(reranking_results):\n",
    "#     print(f\"\\nReranking results for Task {i + 1} ({len(rerank_result)} queries). Here's an example of the top 5 documents for the first query:\")\n",
    "\n",
    "#     for q_id, result in rerank_result.items():\n",
    "#         print(f\"\\nQuery ID: {q_id}\")\n",
    "        \n",
    "#         sorted_results = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#         for i, (doc_id, score) in enumerate(sorted_results[:5]):\n",
    "#             print(f\"  Document {i + 1}: Document ID = {doc_id}, Score = {score}\")\n",
    "\n",
    "#         break  # Only show the first query for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:NDCG@10: 0.4805\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     17\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvFinQA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinanceBench\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTATQA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qrel, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qrels, tasks):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# 평가지표 평가 (retrieval 결과)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#retrieval_metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 평가지표 평가 (rerank 결과)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     rerank_metrics \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(qrels\u001b[38;5;241m=\u001b[39mqrel, results\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mrerank_results, k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# retrieval_ndcg_values.append(retrieval_metrics[0]['NDCG@10'])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# retrieval_map_values.append(retrieval_metrics[1]['MAP@10'])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# retrieval_recall_values.append(retrieval_metrics[2]['Recall@10'])\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# retrieval_precision_values.append(retrieval_metrics[3]['P@10'])\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     rerank_ndcg_values\u001b[38;5;241m.\u001b[39mappend(rerank_metrics[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG@10\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:448\u001b[0m, in \u001b[0;36mBaseTask.evaluate\u001b[0;34m(qrels, results, k_values, ignore_identical_ids)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Compute the average scores for each k\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[0;32m--> 448\u001b[0m     ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    449\u001b[0m     _map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(_map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    450\u001b[0m     recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 각 평가지표를 저장할 리스트\n",
    "retrieval_ndcg_values = []\n",
    "rerank_ndcg_values = []\n",
    "\n",
    "retrieval_map_values = []\n",
    "rerank_map_values = []\n",
    "\n",
    "retrieval_recall_values = []\n",
    "rerank_recall_values = []\n",
    "\n",
    "retrieval_precision_values = []\n",
    "rerank_precision_values = []\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "    # 평가지표 평가 (retrieval 결과)\n",
    "    #retrieval_metrics = task.evaluate(qrels=qrel, results=task.retrieve_results, k_values=[10])\n",
    "    \n",
    "    # 평가지표 평가 (rerank 결과)\n",
    "    rerank_metrics = task.evaluate(qrels=qrel, results=task.rerank_results, k_values=[10])\n",
    "\n",
    "    # retrieval_ndcg_values.append(retrieval_metrics[0]['NDCG@10'])\n",
    "    # retrieval_map_values.append(retrieval_metrics[1]['MAP@10'])\n",
    "    # retrieval_recall_values.append(retrieval_metrics[2]['Recall@10'])\n",
    "    # retrieval_precision_values.append(retrieval_metrics[3]['P@10'])\n",
    "    \n",
    "    rerank_ndcg_values.append(rerank_metrics[0]['NDCG@10'])\n",
    "    rerank_map_values.append(rerank_metrics[1]['MAP@10'])\n",
    "    rerank_recall_values.append(rerank_metrics[2]['Recall@10'])\n",
    "    rerank_precision_values.append(rerank_metrics[3]['P@10'])\n",
    "\n",
    "# 그래프 생성\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# NDCG@10 시각화\n",
    "#plt.subplot(2, 2, 1)\n",
    "#plt.plot(retrieval_ndcg_values, marker='o', label='Retrieval NDCG@10', color='b')\n",
    "plt.plot(rerank_ndcg_values, marker='x', label='Rerank NDCG@10', color='b', linestyle='--')\n",
    "plt.title('NDCG@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# # MAP@10 시각화\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.plot(retrieval_map_values, marker='o', label='Retrieval MAP@10', color='g')\n",
    "# plt.plot(rerank_map_values, marker='x', label='Rerank MAP@10', color='g', linestyle='--')\n",
    "# plt.title('MAP@10 Comparison')\n",
    "# plt.xlabel('Dataset')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Recall@10 시각화\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.plot(retrieval_recall_values, marker='o', label='Retrieval Recall@10', color='r')\n",
    "# plt.plot(rerank_recall_values, marker='x', label='Rerank Recall@10', color='r', linestyle='--')\n",
    "# plt.title('Recall@10 Comparison')\n",
    "# plt.xlabel('Dataset')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Precision@10 시각화\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.plot(retrieval_precision_values, marker='o', label='Retrieval P@10', color='c')\n",
    "# plt.plot(rerank_precision_values, marker='x', label='Rerank P@10', color='c', linestyle='--')\n",
    "# plt.title('Precision@10 Comparison')\n",
    "# plt.xlabel('Dataset')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)  # X축에 데이터셋 이름을 설정\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Reranking (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize CrossEncoder Reranker\n",
    "base_reranker2 = \"BAAI/bge-reranker-v2-m3\" #\"BAAI/bge-reranker-v2-m3\" #\"jinaai/jina-reranker-v2-base-multilingual\" #'cross-encoder/ms-marco-MiniLM-L-12-v2' #\"BAAI/bge-reranker-base\"\n",
    "\n",
    "reranker2 = CrossEncoderReranker(\n",
    "    model=CrossEncoder(base_reranker2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on ConvFinQA Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a557f5614e24c6288a3dd89f4a951ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinBench Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f3ba99e3144a1789a3f531f5a2025f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinDER Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db8440bb8f743959e3af54cc82b534a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinQA Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa7eda400b44cba8aeeb92f4c14ee83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on FinQABench Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defce672ec2a4eb192b92802ecb39620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on MultiHiertt Reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58192c89a1864fb49ad98dfb34403869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.rerank.cross_encoder:Starting To Rerank Top-20....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on TATQA Reranking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e94929aca0477da05334b64a56429a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Perform reranking\n",
    "\n",
    "top_k= 20 # Number of Reranking results\n",
    "batch_size = 8 # 32\n",
    "\n",
    "print(\"\\nWorking on ConvFinQA Reranking\")\n",
    "convfinqa_rerank_second = convfinqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=convfinqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinBench Reranking\")\n",
    "finbench_rerank_second = finbench_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finbench_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinDER Reranking\")\n",
    "finder_rerank_second = finder_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finder_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQA Reranking\")\n",
    "finqa_rerank_second = finqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on FinQABench Reranking\")\n",
    "finqabench_rerank_second = finqabench_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=finqabench_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on MultiHiertt Reranking\")\n",
    "multih_rerank_second = multih_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=multih_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nWorking on TATQA Reranking\")\n",
    "tatqa_rerank_second = tatqa_task.rerank(\n",
    "    reranker=reranker2,\n",
    "    results=tatqa_rerank,\n",
    "    top_k=top_k,  # Rerank the top 100 documents\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "reranking_results_second = [\n",
    "    convfinqa_rerank_second,\n",
    "    finbench_rerank_second,\n",
    "    finder_rerank_second,\n",
    "    finqa_rerank_second,\n",
    "    finqabench_rerank_second,\n",
    "    multih_rerank_second,\n",
    "    tatqa_rerank_second\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:NDCG@10: 0.5377\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvFinQA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinanceBench\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTATQA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qrel, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qrels, tasks):\n\u001b[0;32m---> 19\u001b[0m     rerank_second_metrics \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mevaluate(qrels\u001b[38;5;241m=\u001b[39mqrel, results\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mrerank_results, k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     20\u001b[0m     rerank_second_ndcg_values\u001b[38;5;241m.\u001b[39mappend(rerank_second_metrics[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG@10\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m     rerank_second_map_values\u001b[38;5;241m.\u001b[39mappend(rerank_second_metrics[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAP@10\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/data/DSC/Financerag/financerag/tasks/Base_Task.py:441\u001b[0m, in \u001b[0;36mBaseTask.evaluate\u001b[0;34m(qrels, results, k_values, ignore_identical_ids)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# Compute the average scores for each k\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[0;32m--> 441\u001b[0m     ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(ndcg[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    442\u001b[0m     _map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(_map[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    443\u001b[0m     recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(recall[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rerank_second_ndcg_values = []\n",
    "rerank_second_map_values = []\n",
    "rerank_second_recall_values = []\n",
    "rerank_second_precision_values = []\n",
    "\n",
    "dataset_names = [\n",
    "    'ConvFinQA',\n",
    "    'FinanceBench',\n",
    "    'FinDER',\n",
    "    'FinQA',\n",
    "    'FinQABench',\n",
    "    'MultiHeirtt',\n",
    "    'TATQA'\n",
    "]\n",
    "\n",
    "for qrel, task in zip(qrels, tasks):\n",
    "    rerank_second_metrics = task.evaluate(qrels=qrel, results=task.rerank_results, k_values=[10])\n",
    "    rerank_second_ndcg_values.append(rerank_second_metrics[0]['NDCG@10'])\n",
    "    rerank_second_map_values.append(rerank_second_metrics[1]['MAP@10'])\n",
    "    rerank_second_recall_values.append(rerank_second_metrics[2]['Recall@10'])\n",
    "    rerank_second_precision_values.append(rerank_second_metrics[3]['P@10'])\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(retrieval_ndcg_values, marker='o', label='Retrieval NDCG@10', color='b')\n",
    "plt.plot(rerank_ndcg_values, marker='x', label='Rerank NDCG@10', color='b', linestyle='--')\n",
    "plt.plot(rerank_second_ndcg_values, marker='s', label='Rerank Second NDCG@10', color='b', linestyle='-.')\n",
    "plt.title('NDCG@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(retrieval_map_values, marker='o', label='Retrieval MAP@10', color='g')\n",
    "plt.plot(rerank_map_values, marker='x', label='Rerank MAP@10', color='g', linestyle='--')\n",
    "plt.plot(rerank_second_map_values, marker='s', label='Rerank Second MAP@10', color='g', linestyle='-.')\n",
    "plt.title('MAP@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(retrieval_recall_values, marker='o', label='Retrieval Recall@10', color='r')\n",
    "plt.plot(rerank_recall_values, marker='x', label='Rerank Recall@10', color='r', linestyle='--')\n",
    "plt.plot(rerank_second_recall_values, marker='s', label='Rerank Second Recall@10', color='r', linestyle='-.')\n",
    "plt.title('Recall@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(retrieval_precision_values, marker='o', label='Retrieval P@10', color='c')\n",
    "plt.plot(rerank_precision_values, marker='x', label='Rerank P@10', color='c', linestyle='--')\n",
    "plt.plot(rerank_second_precision_values, marker='s', label='Rerank Second P@10', color='c', linestyle='-.')\n",
    "plt.title('Precision@10 Comparison')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(range(len(dataset_names)), dataset_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reranker 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize CrossEncoder Reranker\n",
    "base_reranker3 = \"BAAI/bge-reranker-v2-m3\" #\"BAAI/bge-reranker-v2-m3\" #\"jinaai/jina-reranker-v2-base-multilingual\" #'cross-encoder/ms-marco-MiniLM-L-12-v2' #\"BAAI/bge-reranker-base\"\n",
    "\n",
    "reranker3 = CrossEncoderReranker(\n",
    "    model=CrossEncoder(base_reranker2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/DSC/Financerag'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/ConvFinQA\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/FinanceBench\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/FinDER\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/FinQA\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/FinQABench\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/MultiHiertt\n",
      "INFO:financerag.tasks.Base_Task:Output directory set to: ./financerag/results/submission_12020100/TAT-QA\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save results\n",
    "results_dir = './financerag/results/'\n",
    "subfolder = 'submission_12020100'\n",
    "\n",
    "output_dir = results_dir+subfolder\n",
    "convfinqa_task.save_results(output_dir=output_dir)\n",
    "finbench_task.save_results(output_dir=output_dir)\n",
    "finder_task.save_results(output_dir=output_dir)\n",
    "finqa_task.save_results(output_dir=output_dir)\n",
    "finqabench_task.save_results(output_dir=output_dir)\n",
    "multih_task.save_results(output_dir=output_dir)\n",
    "tatqa_task.save_results(output_dir=output_dir)\n",
    "\n",
    "csv_files = [\n",
    "    output_dir + '/ConvFinQA/results.csv',\n",
    "    output_dir + '/FinanceBench/results.csv',\n",
    "    output_dir + '/FinDER/results.csv',\n",
    "    output_dir + '/FinQA/results.csv',\n",
    "    output_dir + '/FinQABench/results.csv',\n",
    "    output_dir + '/MultiHiertt/results.csv',\n",
    "    output_dir + '/TAT-QA/results.csv'\n",
    "]\n",
    "\n",
    "results_df = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(results_df, ignore_index=False)\n",
    "combined_df.to_csv(output_dir+'/'+subfolder+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\God_YJ\\\\interns\\\\DS_COMP\\\\FinanceRAG-main'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>corpus_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4b92b32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4ba2a5a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4be1f98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4ba07d2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qd496c6a0</td>\n",
       "      <td>dd4ba02f0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_id  corpus_id\n",
       "0  qd496c6a0  dd4b92b32\n",
       "1  qd496c6a0  dd4ba2a5a\n",
       "2  qd496c6a0  dd4be1f98\n",
       "3  qd496c6a0  dd4ba07d2\n",
       "4  qd496c6a0  dd4ba02f0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id present only in sample:\n",
      "['query_id']\n",
      "query_id present only in combined_df:\n",
      "['q00097', 'qd2abb228', 'q00029']\n"
     ]
    }
   ],
   "source": [
    "# sample과 combined_df에서 고유한 query_id 값 얻기\n",
    "sample_query_ids = sample['query_id'].unique()\n",
    "combined_query_ids = combined_df['query_id'].unique()\n",
    "\n",
    "# sample과 combined_df에만 존재하는 query_id 값 구하기\n",
    "sample_only_query_ids = list(set(sample_query_ids) - set(combined_query_ids))\n",
    "combined_only_query_ids = list(set(combined_query_ids) - set(sample_query_ids))\n",
    "\n",
    "# 결과 출력\n",
    "print(\"query_id present only in sample:\")\n",
    "print(sample_only_query_ids)\n",
    "\n",
    "print(\"query_id present only in combined_df:\")\n",
    "print(combined_only_query_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9677683,
     "sourceId": 85594,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DSC",
   "language": "python",
   "name": "dsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
